import streamlit as st
import pandas as pd
import requests
import zipfile
from tempfile import NamedTemporaryFile

# --------------------------------------------------------------
# üîë Secrets ‚Äì add these in .streamlit/secrets.toml or via UI
# --------------------------------------------------------------
SUPABASE_URL = st.secrets["SUPABASE_URL"].rstrip("/")
SUPABASE_ANON_KEY = st.secrets["SUPABASE_ANON_KEY"]
# Optional ‚Äì needed only for the full‚ÄëSQL‚Äëdump feature
SUPABASE_SERVICE_ROLE_KEY = st.secrets.get("SUPABASE_SERVICE_ROLE_KEY")

# --------------------------------------------------------------
# üì¶ Helper ‚Äì build auth headers
# --------------------------------------------------------------
def _auth_headers(use_service_role: bool = False) -> dict:
    """Return headers for the anon key or the service‚Äërole key."""
    key = SUPABASE_SERVICE_ROLE_KEY if use_service_role else SUPABASE_ANON_KEY
    return {
        "apikey": key,
        "Authorization": f"Bearer {key}",
        "Accept": "application/json",
    }

# --------------------------------------------------------------
# 1Ô∏è‚É£ List every user table in the public schema
# --------------------------------------------------------------
@st.cache_data(ttl=600)  # cache for 10‚ÄØmin
def list_user_tables() -> list[str]:
    """Return a list of table names in the public schema."""
    sql = """
        SELECT table_name
        FROM information_schema.tables
        WHERE table_schema = 'public'
          AND table_type = 'BASE TABLE';
    """
    resp = requests.post(
        f"{SUPABASE_URL}/rest/v1/rpc",
        json={"query": sql, "params": []},
        headers=_auth_headers(),
        timeout=30,
    )
    resp.raise_for_status()
    rows = resp.json()
    return [r["table_name"] for r in rows]

# --------------------------------------------------------------
# 2Ô∏è‚É£ Paginated fetch for a single table (Range header)
# --------------------------------------------------------------
def fetch_table_paginated(table: str, chunk: int = 10_000) -> pd.DataFrame:
    """
    Pull *all* rows from `table` using Supabase‚Äôs Range-Unit header.
    The loop ends when the server returns HTTP‚ÄØ200 (final page) or
    when the declared total row count is reached.
    """
    all_rows = []
    start = 0

    while True:
        end = start + chunk - 1
        headers = _auth_headers()
        headers.update(
            {
                "Range-Unit": "items",      # crucial for correct pagination
                "Range": f"{start}-{end}",  # fetch the slice we need
            }
        )
        url = f"{SUPABASE_URL}/rest/v1/{table}"
        resp = requests.get(url, headers=headers, timeout=300)

        # 206 = partial content (more pages); 200 = final page
        if resp.status_code not in (200, 206):
            resp.raise_for_status()

        batch = resp.json()
        if not batch:
            break   # empty table

        all_rows.extend(batch)

        if resp.status_code == 200:
            # Last page ‚Äì we‚Äôre done
            break

        # Optional: stop early if we know the total size
        content_range = resp.headers.get("Content-Range")
        if content_range:
            try:
                _, range_part = content_range.split(" ")
                _, total = range_part.split("/")
                total = int(total)
                if start + chunk >= total:
                    break
            except Exception:
                pass  # parsing failure ‚Äì just continue

        start += chunk

    return pd.DataFrame(all_rows)

# --------------------------------------------------------------
# 3Ô∏è‚É£ OPTIONAL: Full SQL dump (service‚Äërole only)
# --------------------------------------------------------------
def download_sql_dump() -> bytes:
    """
    Calls Supabase‚Äôs hidden pg_dump RPC.
    Returns the raw SQL (base‚Äë64 decoded).
    """
    # Extract the project reference from the URL (e.g. xyz.supabase.co ‚Üí xyz)
    ref = SUPABASE_URL.split("/")[-1].split(".")[0]
    dump_url = f"https://{ref}.supabase.co/rest/v1/rpc/pg_dump"

    resp = requests.post(
        dump_url,
        json={},
        headers=_auth_headers(use_service_role=True),
        timeout=600,
    )
    resp.raise_for_status()
    import base64

    payload = resp.json()
    b64 = payload.get("dump") or payload.get("data")
    return base64.b64decode(b64)

# --------------------------------------------------------------
# üé® UI
# --------------------------------------------------------------
st.title("üóÇÔ∏è Supabase ‚Äì Export Every Table (Full Data)")

# -----------------------------------------------------------------
# Step‚ÄØ1 ‚Äì discover tables (or keep manual list if you prefer)
# -----------------------------------------------------------------
with st.spinner("Fetching list of tables‚Ä¶"):
    discovered_tables = list_user_tables()

if not discovered_tables:
    st.error("‚ùå No tables found in the `public` schema.")
    st.stop()

st.success(f"‚úÖ Found **{len(discovered_tables)}** tables.")
st.caption(", ".join(discovered_tables))

# -----------------------------------------------------------------
# Step‚ÄØ2 ‚Äì download CSV‚ÄØ+‚ÄØSQL ZIP
# -----------------------------------------------------------------
if st.button("Export ALL tables as CSV‚ÄØ+‚ÄØSQL ZIP"):
    with st.spinner("Downloading tables ‚Äì this can take a while‚Ä¶"):
        with NamedTemporaryFile(delete=False, suffix=".zip") as tmp_zip:
            with zipfile.ZipFile(tmp_zip, "w") as zf:
                for tbl in discovered_tables:
                    st.info(f"üì• Fetching **{tbl}** ‚Ä¶")
                    try:
                        df = fetch_table_paginated(tbl)
                        if df.empty:
                            st.warning(f"‚ö†Ô∏è `{tbl}` appears empty.")
                        csv_bytes = df.to_csv(index=False).encode("utf‚Äë8")
                        zf.writestr(f"{tbl}.csv", csv_bytes)
                        st.success(f"‚úÖ `{tbl}` ({len(df)} rows) added to ZIP.")
                    except Exception as e:
                        st.warning(f"‚ùå Failed to fetch `{tbl}`: {e}")

                # -------------------------------------------------
                # Optional: full‚ÄëSQL dump (requires service‚Äërole key)
                # -------------------------------------------------
                if SUPABASE_SERVICE_ROLE_KEY:
                    try:
                        st.info("Generating full SQL dump ‚Ä¶")
                        dump_bytes = download_sql_dump()
                        zf.writestr("database.sql", dump_bytes)
                        st.success("‚úÖ SQL dump added to ZIP.")
                    except Exception as exc:
                        st.warning(f"‚ö†Ô∏è Could not create SQL dump: {exc}")

            tmp_zip.flush()
            tmp_zip.seek(0)

            with open(tmp_zip.name, "rb") as f:
                st.download_button(
                    label="‚¨áÔ∏è Download ZIP (CSV‚ÄØ+‚ÄØSQL dump)",
                    data=f,
                    file_name="supabase_export.zip",
                    mime="application/zip",
                )
    st.success("‚úÖ Export ready!")

# -----------------------------------------------------------------
# Step‚ÄØ3 ‚Äì pure‚ÄëSQL dump (service‚Äërole only)
# -----------------------------------------------------------------
if SUPABASE_SERVICE_ROLE_KEY:
    if st.button("Download ONLY full SQL dump (service‚Äërole)"):
        with st.spinner("Creating SQL dump‚Ä¶"):
            try:
                dump_bytes = download_sql_dump()
                st.download_button(
                    label="‚¨áÔ∏è Download database.sql",
                    data=dump_bytes,
                    file_name="database.sql",
                    mime="application/sql",
                )
                st.success("‚úÖ SQL dump ready!")
            except Exception as exc:
                st.error(f"‚ùå Failed to generate dump: {exc}")

# -----------------------------------------------------------------
# Info panel
# -----------------------------------------------------------------
st.info(
    """
- **Read‚Äëonly**: Only GET requests (or the optional service‚Äërole RPC) are made.  
- **RLS**: When using the anon key you only receive rows the client is allowed to see.  
- **Full dump**: Requires `SUPABASE_SERVICE_ROLE_KEY`; it bypasses RLS and returns the exact DB state.  
- **No external services** ‚Äì everything runs locally in the Streamlit process.  
"""
)
